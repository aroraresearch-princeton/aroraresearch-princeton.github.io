<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Arora Research Lab | Arora Research Lab @ Princeton</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Arora Research Lab" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Arora Research Lab @ Princeton" />
<meta property="og:description" content="Arora Research Lab @ Princeton" />
<link rel="canonical" href="http://localhost:4000/pages/publications.html" />
<meta property="og:url" content="http://localhost:4000/pages/publications.html" />
<meta property="og:site_name" content="Arora Research Lab" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Arora Research Lab" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Arora Research Lab @ Princeton","headline":"Arora Research Lab","url":"http://localhost:4000/pages/publications.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/">Arora Research Lab</a></h1>

        

        <p>Arora Research Lab @ Princeton</p>

        <p><a href="http://localhost:4000/pages/team.html">Team</a>&emsp;/&emsp;<a
            href="http://localhost:4000/pages/publications.html">Publications</a>&emsp;/&emsp;<a href="http://localhost:4000/pages/blogs.html">Blogs</a></p>
      </header>
      <section>

      <h2 id="publications">Publications</h2>

<p>Please also check our google scholar pages and arxiv for latest works</p>

<h2 id="2025">2025</h2>

<div class="publications">
    <ul>
        
        

        
        <li><a href="https://arxiv.org/abs/2408.14774">Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning</a>. ICLR 2025, Compositional Learning Workshop at NeurIPS 2024 (Oral), FITML Workshop at NeurIPS 2024 (Simran Kaur, Simon Park, Anirudh Goyal, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2411.12600">Provable Unlearning in Topic Modeling and Downstream Tasks</a>. ICLR 2025 (Stanley Wei, Sadhika Malladi, Sanjeev Arora, Amartya Sanyal)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2410.08847">Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization</a>. ICLR 2025, FITML Workshop at NeurIPS 2024, ATTRIB Workshop at NeurIPS 2024 (Noam Razin, Sadhika Malladi, Adithya Bhaskar, Danqi Chen, Sanjeev Arora, Boris Hanin)</li>
        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
    </ul>
</div>

<h2 id="2024">2024</h2>

<div class="publications">
    <ul>
        
        

        
        
        

        
        
        

        
        
        

        
        <li><a href="https://arxiv.org/abs/2402.04333">Less: Selecting influential data for targeted instruction tuning</a>. ICML 2024 (Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi Chen)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2405.12205">Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving</a>. NeurIPS 2024 (Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2402.18540">Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates</a>. NeurIPS 2024 (Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2406.18521">CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs</a>. NeurIPS 2024 (Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, Danqi Chen)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2402.11111">Language Models as Science Tutors</a>. ICML 2024 (Alexis Chevalier, Jiayi Geng, Alexander Wettig, Howard Chen, Sebastian Mizera, Toni Annala, Max Jameson Aragon, Arturo Rodr√≠guez Fanlo, Simon Frieder, Simon Machado, Akshara Prabhakar, Ellie Thieu, Jiachen T Wang, Zirui Wang, Xindi Wu, Mengzhou Xia, Wenhan Jia, Jiatong Yu, Jun-Jie Zhu, Zhiyong Jason Ren, Sanjeev Arora, Danqi Chen)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2407.21009">AI-Assisted Generation of Difficult Math Questions</a>. MATH-AI Workshop at NeurIPS 2024 (Vedant Shah, Dingli Yu, Kaifeng Lyu, Simon Park, Nan Rosemary Ke, Michael Mozer, Yoshua Bengio, Sanjeev Arora, Anirudh Goyal)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2408.14339">ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty</a>. NeurIPS 2024 Datasets Track (Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2409.19808">Can Models Learn Skill Composition from Examples?</a>. NeurIPS 2024 (Haoyu Zhao, Simran Kaur, Dingli Yu, Anirudh Goyal, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2410.05464">Progressive Distillation Induces an Implicit Curriculum</a>. M3L Workshop at NeurIPS 2024, Theoretical Foundations of Foundation Models Workshop at ICML 2024, MI Workshop at ICML 2024 (Abhishek Panigrahi, Bingbin Liu, Sadhika Malladi, Andrej Risteski, Surbhi Goel)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2405.19534">Preference Learning Algorithms Do Not Learn Preference Rankings</a>. NeurIPS 2024 (Angelica Chen, Sadhika Malladi, Lily H. Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, Kyunghyun Cho)</li>
        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
    </ul>
</div>

<h2 id="2023">2023</h2>

<div class="publications">
    <ul>
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a627810151be4d13f907ac898ff7e948-Abstract-Conference.html">Fine-Tuning Language Models with Just Forward Passes</a>. NeurIPS 2023 (Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2307.15936">A Theory for Emergence of Complex Skills in Language Models</a>. Preprint (Sanjeev Arora, Anirudh Goyal)</li>
        
        
        

        
        <li><a href="https://proceedings.mlr.press/v202/panigrahi23a.html">Task-Specific Skill Localization in Fine-tuned Language Models</a>. ICML 2023 (Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://proceedings.mlr.press/v202/malladi23a.html">A Kernel-Based View of Language Model Fine-Tuning</a>. ICML 2023 (Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2310.17567">Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models</a>. ICLR 2024 (Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2307.15196">The Marginal Value of Momentum for Small Learning Rate SGD</a>. ICLR 2024 (Runzhe Wang, Sadhika Malladi, Tianhao Wang, Kaifeng Lyu, Zhiyuan Li)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2303.01215">Why (and When) does Local SGD Generalize Better than SGD?</a>. ICLR 2023 (Xinran Gu, Kaifeng Lyu, Longbo Huang, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2307.01189">Trainable Transformer in Transformer</a>. ICML 2024 (Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2311.15268">Unlearning via Sparse Representations</a>. Preprint (Vedant Shah, Frederik Tr√§uble, Ashish Malik, Hugo Larochelle, Michael Mozer, Sanjeev Arora, Yoshua Bengio, Anirudh Goyal)</li>
        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
    </ul>
</div>

<h2 id="2022">2022</h2>

<div class="publications">
    <ul>
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        <li><a href="https://proceedings.mlr.press/v162/saunshi22a.html">Understanding Contrastive Learning Requires Incorporating Inductive Biases</a>. ICML 2022 (Nikunj Saunshi, Jordan Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev Arora, Sham Kakade, Akshay Krishnamurthy)</li>
        
        
        

        
        <li><a href="https://proceedings.mlr.press/v162/arora22a.html">Understanding Gradient Descent on the Edge of Stability in Deep Learning</a>. ICML 2022 (Sanjeev Arora, Zhiyuan Li, Abhishek Panigrahi)</li>
        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/dffd1c523512e557f4e75e8309049213-Abstract-Conference.html">Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction</a>. NeurIPS 2022 (Kaifeng Lyu, Zhiyuan Li, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/32ac710102f0620d0f28d5d05a44fe08-Abstract-Conference.html">On the SDEs and Scaling Rules for Adaptive Gradient Algorithms</a>. NeurIPS 2022 (Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2210.01072">Understanding Influence Functions and Datamodels via Harmonic Analysis</a>. ICLR 2023 (Nikunj Saunshi, Arushi Gupta, Mark Braverman, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2203.01400">Adaptive Gradient Methods with Local Guarantees</a>. Preprint (Zhou Lu, Wenhan Xia, Sanjeev Arora, Elad Hazan)</li>
        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/d6383e7643415842b48a5077a1b09c98-Abstract-Conference.html">New Definitions and Evaluations for Saliency Methods: Staying Intrinsic, Complete and Sound</a>. NeurIPS 2022 (Arushi Gupta, Nikunj Saunshi, Dingli Yu, Kaifeng Lyu, Sanjeev Arora)</li>
        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
    </ul>
</div>

<h2 id="2021">2021</h2>

<div class="publications">
    <ul>
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/3b3fff6463464959dcd1b68d0320f781-Abstract.html">Evaluating Gradient Inversion Attacks and Defenses in Federated Learning</a>. NeurIPS 2021 (Yangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2110.06914">What Happens after SGD Reaches Zero Loss?--A Mathematical Framework</a>. NeurIPS 2021, (Zhiyuan Li, Tianhao Wang, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/6c351da15b5e8a743a21ee96a86e25df-Abstract.html">Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias</a>. NeurIPS 2021 (Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper/2021/hash/69f62956429865909921fa916d61c1f8-Abstract.html">On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)</a>. NeurIPS 2021 (Zhiyuan Li, Sadhika Malladi, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2111.14212">On Predicting Generalization using GANs</a>. ICLR 2022 (spotlight) (Yi Zhang, Arushi Gupta, Nikunj Saunshi, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2102.13189">Rip van Winkle's Razor: A Simple Estimate of Overfit to Test Data</a>. Preprint (Sanjeev Arora, Yi Zhang)</li>
        
        
        

        
        <li><a href="https://dl.acm.org/doi/abs/10.1145/3410220.3453910">Opening the Black Box of Deep Learning: Some Lessons and Take-aways</a>. SIGMETRICS '21 (Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://dl.acm.org/doi/abs/10.1145/3446773">Technical perspective: Why don't today's deep nets overfit to their training data?</a>. Communications of the ACM (Sanjeev Arora)</li>
        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
    </ul>
</div>

<h2 id="2020">2020</h2>

<div class="publications">
    <ul>
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        <li><a href="http://proceedings.mlr.press/v119/huang20i.html">InstaHide: Instance-hiding Schemes for Private Distributed Learning</a>. ICML 2020 (Yangsibo Huang, Zhao Song, Kai Li, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2010.03648">A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks</a>. ICLR 2021 (Nikunj Saunshi, Sadhika Malladi, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper/2020/hash/a7453a5f026fb6831d68bdc9cb0edcae-Abstract.html">Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate</a>. NeurIPS 2020 (Zhiyuan Li, Kaifeng Lyu, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://proceedings.mlr.press/v119/arora20a.html">Provable Representation Learning for Imitation Learning via Bi-level Optimization</a>. ICML 2020 (Sanjeev Arora, Simon Du, Sham Kakade, Yuping Luo, Nikunj Saunshi)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2010.08515">Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?</a>. ICLR 2021 (Zhiyuan Li, Yi Zhang, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://aclanthology.org/2020.findings-emnlp.123/">TextHide: Tackling Data Privacy in Language Understanding Tasks</a>. EMNLP 2020 (Findings) (Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper_files/paper/2020/hash/0740bb92e583cd2b88ec7c59f985cb41-Abstract.html">Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality</a>. NeurIPS 2020 (Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="http://proceedings.mlr.press/v119/saunshi20a.html">A Sample Complexity Separation between Non-Convex and Convex Meta-Learning</a>. ICML 2020 (Nikunj Saunshi, Yi Zhang, Mikhail Khodak, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/2003.01876">Privacy-preserving Learning via Deep Net Pruning</a>. Preprint (Yangsibo Huang, Yushan Su, Sachin Ravi, Zhao Song, Sanjeev Arora, Kai Li)</li>
        
        
        

        
        <li><a href="https://www.cs.princeton.edu/~arora/TheoryDL.pdf">Theory of deep learning</a>. Manuscript (Raman Arora, Sanjeev Arora, Joan Bruna, Nadav Cohen, Simon Du, Rong Ge, Suriya Gunasekar, Chi Jin, Jason Lee, Tengyu Ma, Behnam Neyshabur, Zhao Song)</li>
        
        
        

        
        <li><a href="https://drops.dagstuhl.de/search/documents?author=Arora,%20Sanjeev">The Quest for Mathematical Understanding of Deep Learning</a>. 40th IARCS Annual Conference on Foundations of Software Technology (Sanjeev Arora)</li>
        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
    </ul>
</div>

<h2 id="2019">2019</h2>

<div class="publications">
    <ul>
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        <li><a href="http://proceedings.mlr.press/v97/arora19a.html">Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks</a>. ICML 2019 (Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, Ruosong Wang)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/1904.11955">On Exact Computation with an Infinitely Wide Neural Net</a>. NeurIPS 2019 (Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang)</li>
        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper/2019/hash/c0c783b5fc0d7d808f1d14a6e9c8280d-Abstract.html">Implicit Regularization in Deep Matrix Factorization</a>. NeurIPS 2019 (Sanjeev Arora, Nadav Cohen, Wei Hu, Yuping Luo)</li>
        
        
        

        
        <li><a href="http://proceedings.mlr.press/v97/saunshi19a">A Theoretical Analysis of Contrastive Unsupervised Representation Learning</a>. ICML 2019 (Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, Hrishikesh Khandeparkar)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/1910.07454">An Exponential Learning Rate Schedule for Deep Learning</a>. ICLR 2021 (Zhiyuan Li, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/1910.01663">Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks</a>. ICLR 2020 (Sanjeev Arora, Simon S Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, Dingli Yu)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/1911.00809">Enhanced Convolutional Neural Tangent Kernels</a>. Preprint (Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://proceedings.neurips.cc/paper/2019/hash/46a4378f835dc8040c8057beb6a2da52-Abstract.html">Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets</a>. NeurIPS 2019 (Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, Rong Ge)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/1810.02281">A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks</a>. ICLR 2019 (Sanjeev Arora, Nadav Cohen, Noah Golowich, Wei Hu)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/1905.12152">A Simple Saliency Method That Passes the Sanity Checks</a>. Preprint (Arushi Gupta, Sanjeev Arora)</li>
        
        
        

        
        <li><a href="https://arxiv.org/abs/1910.01663">Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks</a>. ICLR 2020 (spotlight) (Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, Dingli Yu)</li>
        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
    </ul>
</div>

<p><a href="/index.html">back</a></p>


      </section>
      <footer>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
